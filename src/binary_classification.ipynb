{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "# %load preprocessing.py\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english')) - set(('not', 'no'))\n",
    "\n",
    "\n",
    "tags = ['<url>', '<email>', '<user>', '<hashtag>', '</hashtag>',\n",
    "        '<elongated>', '</elongated>', '<repeated>', '</repeated>']\n",
    "\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'user'],\n",
    "    annotate={'hashtag', 'elongated', 'repeated'},\n",
    "    segmenter=\"twitter\",\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,\n",
    "    unpack_contractions=True,\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    txt = text_processor.pre_process_doc(text)\n",
    "    return list(filter(lambda x: x not in tags and\n",
    "                                 x not in stopwords and\n",
    "                                 x not in punctuation, txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load load.py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "label2emotion = {0:\"others\", 1:\"happy\", 2:\"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}\n",
    "\n",
    "emotion2label_angry = {\"others\":0, \"happy\":0, \"sad\":0, \"angry\":3}\n",
    "emotion2label_sad = {\"others\":0, \"happy\":0, \"sad\":2, \"angry\":0}\n",
    "emotion2label_happy = {\"others\":0, \"happy\":1, \"sad\":0, \"angry\":0}\n",
    "\n",
    "\n",
    "def load_data(path, training):\n",
    "    data = pd.read_csv(path, encoding='utf-8', sep='\\t')\n",
    "    text = data[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    if not training:\n",
    "        return data['id'], text\n",
    "    else:\n",
    "        return data['id'], text, data['label']\n",
    "\n",
    "\n",
    "def load_preprocessed_data(path, training=True):\n",
    "    if not training:\n",
    "        id, text = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        return id.values.tolist(), t.values.tolist()\n",
    "    else:\n",
    "        id, text, label = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        l = label.apply(lambda x: emotion2label[x])\n",
    "        return id.values.tolist(), t.values.tolist(), l.values.tolist()\n",
    "\n",
    "    \n",
    "def load_preprocessed_data_angry(path, training=True):\n",
    "    if not training:\n",
    "        id, text = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        return id.values.tolist(), t.values.tolist()\n",
    "    else:\n",
    "        id, text, label = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        l = label.apply(lambda x: emotion2label_angry[x])\n",
    "        return id.values.tolist(), t.values.tolist(), l.values.tolist()\n",
    "\n",
    "    \n",
    "def load_preprocessed_data_sad(path, training=True):\n",
    "    if not training:\n",
    "        id, text = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        return id.values.tolist(), t.values.tolist()\n",
    "    else:\n",
    "        id, text, label = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        l = label.apply(lambda x: emotion2label_sad[x])\n",
    "        return id.values.tolist(), t.values.tolist(), l.values.tolist()\n",
    "\n",
    "\n",
    "def load_preprocessed_data_happy(path, training=True):\n",
    "    if not training:\n",
    "        id, text = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        return id.values.tolist(), t.values.tolist()\n",
    "    else:\n",
    "        id, text, label = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        l = label.apply(lambda x: emotion2label_happy[x])\n",
    "        return id.values.tolist(), t.values.tolist(), l.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Concatenate, Reshape, GRU, Bidirectional, Dropout, Conv1D, Flatten, MaxPool1D, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from train import create_solution_file\n",
    "np.random.seed(7)\n",
    "\n",
    "trainDataPath = '../data/starterkit/train.txt'\n",
    "testDataPath = '../data/starterkit/devwithoutlabels.txt'\n",
    "validationDataPath = '../data/devsetwithlabels.txt'\n",
    "solutionPath = 'test.txt'\n",
    "gloveDir = '../data/'\n",
    "#NUM_CLASSES = 4 # for angry\n",
    "#NUM_CLASSES = 3 # for sad\n",
    "NUM_CLASSES = 2 # for happy\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 64\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 200\n",
    "LSTM_DIM = 300\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "DROPOUT = 0.2\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        r = self.model.predict(x)\n",
    "        getMetrics(r, y)\n",
    "        \n",
    "def create_solution_file(model,u_testSequences):\n",
    "    u_testData = pad_sequences(u_testSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions = model.predict(u_testData, batch_size=BATCH_SIZE)\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "\n",
    "    with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')\n",
    "        with io.open(testDataPath, encoding=\"utf8\") as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "                fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "    print(\"Completed. Model parameters: \")\n",
    "    print(\"Learning rate : %.3f, LSTM Dim : %d, Dropout : %.3f, Batch_size : %d\"\n",
    "          % (LEARNING_RATE, LSTM_DIM, DROPOUT, BATCH_SIZE))\n",
    "    return\n",
    "        \n",
    "def getEmbeddingMatrix(wordIndex):\n",
    "    embeddingsIndex = {}\n",
    "    with io.open(os.path.join(gloveDir, 'glove.840B.300d.txt'), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embeddingVector = np.array([float(val) for val in values[1:]])\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "        else:\n",
    "            oov = [np.random.normal(size = EMBEDDING_DIM)]\n",
    "            oov /= np.linalg.norm(oov)\n",
    "            embeddingMatrix[i] = oov\n",
    "\n",
    "    return embeddingMatrix\n",
    "\n",
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "    \n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "    \n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "    \n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "    \n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()    \n",
    "    \n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "    \n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "    \n",
    "    return accuracy, microPrecision, microRecall, microF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n",
      "Processing validation data...\n"
     ]
    }
   ],
   "source": [
    "def model10(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(GRU(LSTM_DIM, return_sequences=True))\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "print(\"Processing training data...\")\n",
    "#trainIndices, text_train, labels = load_preprocessed_data_angry(trainDataPath)\n",
    "#trainIndices, text_train, labels = load_preprocessed_data_sad(trainDataPath)\n",
    "trainIndices, text_train, labels = load_preprocessed_data_happy(trainDataPath)\n",
    "print(\"Processing test data...\")\n",
    "_, text_test = load_preprocessed_data(testDataPath, training=False)\n",
    "print(\"Processing validation data...\")\n",
    "#_, X_validation, y_validation = load_preprocessed_data_angry(validationDataPath)\n",
    "#_, X_validation, y_validation = load_preprocessed_data_sad(validationDataPath)\n",
    "_, X_validation, y_validation = load_preprocessed_data_happy(validationDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokens...\n",
      "Found 14162 unique tokens.\n",
      "Populating embedding matrix...\n",
      "Found 2196016 word vectors.\n",
      "labels\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "labels_validation\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "u_trainSequences = tokenizer.texts_to_sequences(text_train)\n",
    "u_testSequences = tokenizer.texts_to_sequences(text_test)\n",
    "u_validationSequences = tokenizer.texts_to_sequences(X_validation)\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "print(\"Populating embedding matrix...\")\n",
    "embeddingMatrix = getEmbeddingMatrix(wordIndex)\n",
    "u_data = pad_sequences(u_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "u_validation = pad_sequences(u_validationSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_validation = to_categorical(np.asarray(y_validation))\n",
    "print(\"labels\")\n",
    "print(labels)\n",
    "print(\"labels_validation\")\n",
    "print(labels_validation)\n",
    "np.random.shuffle(trainIndices)\n",
    "u_data = u_data[trainIndices]\n",
    "labels = labels[trainIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 64, 300)           4248900   \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 64, 300)           540900    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 64, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 32, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 4,820,682\n",
      "Trainable params: 4,820,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30160 samples, validate on 2755 samples\n",
      "Epoch 1/10\n",
      "30160/30160 [==============================] - 146s 5ms/step - loss: 0.2243 - acc: 0.9102 - val_loss: 0.1062 - val_acc: 0.9630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10620, saving model to ./model_sad.h5\n",
      "True Positives per class :  [2551.  102.]\n",
      "False Positives per class :  [40. 62.]\n",
      "False Negatives per class :  [62. 40.]\n",
      "Class happy : Precision : 0.622, Recall : 0.718, F1 : 0.667\n",
      "Ignoring the Others class, Macro Precision : 0.2073, Macro Recall : 0.2394, Macro F1 : 0.2222\n",
      "Ignoring the Others class, Micro TP : 102, FP : 62, FN : 40\n",
      "Accuracy : 0.9630, Micro Precision : 0.6220, Micro Recall : 0.7183, Micro F1 : 0.6667\n",
      "Epoch 2/10\n",
      "30160/30160 [==============================] - 146s 5ms/step - loss: 0.1200 - acc: 0.9544 - val_loss: 0.1023 - val_acc: 0.9659\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10620 to 0.10227, saving model to ./model_sad.h5\n",
      "True Positives per class :  [2565.   96.]\n",
      "False Positives per class :  [46. 48.]\n",
      "False Negatives per class :  [48. 46.]\n",
      "Class happy : Precision : 0.667, Recall : 0.676, F1 : 0.671\n",
      "Ignoring the Others class, Macro Precision : 0.2222, Macro Recall : 0.2254, Macro F1 : 0.2238\n",
      "Ignoring the Others class, Micro TP : 96, FP : 48, FN : 46\n",
      "Accuracy : 0.9659, Micro Precision : 0.6667, Micro Recall : 0.6761, Micro F1 : 0.6713\n",
      "Epoch 3/10\n",
      "30160/30160 [==============================] - 145s 5ms/step - loss: 0.0905 - acc: 0.9656 - val_loss: 0.1488 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.10227\n",
      "True Positives per class :  [2500.  110.]\n",
      "False Positives per class :  [ 32. 113.]\n",
      "False Negatives per class :  [113.  32.]\n",
      "Class happy : Precision : 0.493, Recall : 0.775, F1 : 0.603\n",
      "Ignoring the Others class, Macro Precision : 0.1644, Macro Recall : 0.2582, Macro F1 : 0.2009\n",
      "Ignoring the Others class, Micro TP : 110, FP : 113, FN : 32\n",
      "Accuracy : 0.9474, Micro Precision : 0.4933, Micro Recall : 0.7746, Micro F1 : 0.6027\n",
      "Epoch 4/10\n",
      "30160/30160 [==============================] - 148s 5ms/step - loss: 0.0686 - acc: 0.9760 - val_loss: 0.1538 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10227\n",
      "True Positives per class :  [2522.  106.]\n",
      "False Positives per class :  [36. 91.]\n",
      "False Negatives per class :  [91. 36.]\n",
      "Class happy : Precision : 0.538, Recall : 0.746, F1 : 0.625\n",
      "Ignoring the Others class, Macro Precision : 0.1794, Macro Recall : 0.2488, Macro F1 : 0.2085\n",
      "Ignoring the Others class, Micro TP : 106, FP : 91, FN : 36\n",
      "Accuracy : 0.9539, Micro Precision : 0.5381, Micro Recall : 0.7465, Micro F1 : 0.6254\n",
      "Creating solution file...\n",
      "Completed. Model parameters: \n",
      "Learning rate : 0.001, LSTM Dim : 300, Dropout : 0.200, Batch_size : 200\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model...\")\n",
    "cbks = [ModelCheckpoint('./model_sad.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=2),\n",
    "        TestCallback((u_validation, labels_validation))]\n",
    "model = model10(embeddingMatrix)\n",
    "model.fit(u_data, labels, validation_data=(u_validation, labels_validation), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=cbks)\n",
    "model = load_model('./model_sad.h5')\n",
    "print(\"Creating solution file...\")\n",
    "create_solution_file(model, u_testSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
